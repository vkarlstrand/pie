{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb3e06f",
   "metadata": {},
   "source": [
    "# Adversarial Attack and Detection in Medical Images using Deap Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc3339",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Import common libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db3b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9994709b",
   "metadata": {},
   "source": [
    "## Load non-attacked and attacked images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015220d",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2181079",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATT_LOAD_PARTITION_ROOT = './attacked_images_case_2/'      # Path to load images from\n",
    "ATT_LOAD_PARTITION = './attacked_images_case_2/2021-11-25' # Path to load .csv files from\n",
    "BATCH_SIZE = 16                                            # Batch size to to use\n",
    "NUM_WORKERS = 1                                            # Number of workers to use\n",
    "IMAGE_SIZE = 400                                           # Image size to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2cae0",
   "metadata": {},
   "source": [
    "#### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e9277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as album\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from data import LoadDatasetFromCSV\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "\n",
    "\n",
    "def concat_datasets(dataset, dataset_attacked):\n",
    "    \"\"\"\n",
    "    Concatenate non-attacked images and attacked images to a dataset, with true labels from non-attacked images copied \n",
    "    to attacked dataset also.\n",
    "    \"\"\"\n",
    "    cat_dataset = []\n",
    "    for i, (org, att) in enumerate(zip(dataset, dataset_attacked)):\n",
    "        img_org = org[0]\n",
    "        img_att = att[0]\n",
    "        true_lbl = org[1]\n",
    "        # Use true label for both sicne we want to retrain model to classify attacked images to\n",
    "        cat_dataset.append(tuple([img_org, true_lbl]))\n",
    "        cat_dataset.append(tuple([img_att, true_lbl]))\n",
    "    return cat_dataset\n",
    "        \n",
    "\n",
    "\n",
    "# Pre-computed mean and std\n",
    "data_mean = torch.tensor([0.7750, 0.5888, 0.7629])\n",
    "data_std = torch.tensor([0.2129, 0.2971, 0.1774])\n",
    "\n",
    "# Resize images and rescale values\n",
    "album_compose = album.Compose([\n",
    "    album.Resize(IMAGE_SIZE, IMAGE_SIZE),                                          # Resize to IMAGE_SIZE x IMAGE_SIZE\n",
    "    album.Normalize(mean=[0.0,0.0,0.0], std=[1.0,1.0,1.0], max_pixel_value=255.0), # Rescale values from [0,255] to [0,1]\n",
    "    album.Normalize(mean=data_mean, std=data_std, max_pixel_value=1.0),            # Rescale values according to above\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets for training and test\n",
    "dataset_attacks_train   = LoadDatasetFromCSV(image_root=ATT_LOAD_PARTITION_ROOT,\n",
    "                                             csv_path=ATT_LOAD_PARTITION+'/attacked_images/data_labels_train.csv',\n",
    "                                             transforms=album_compose)\n",
    "dataset_attacks_test    = LoadDatasetFromCSV(image_root=ATT_LOAD_PARTITION_ROOT,\n",
    "                                             csv_path=ATT_LOAD_PARTITION+'/attacked_images/data_labels_test.csv',\n",
    "                                             transforms=album_compose)\n",
    "dataset_originals_train = LoadDatasetFromCSV(image_root=ATT_LOAD_PARTITION_ROOT,\n",
    "                                             csv_path=ATT_LOAD_PARTITION+'/original_images/data_labels_train.csv',\n",
    "                                             transforms=album_compose)\n",
    "dataset_originals_test  = LoadDatasetFromCSV(image_root=ATT_LOAD_PARTITION_ROOT,\n",
    "                                             csv_path=ATT_LOAD_PARTITION+'/original_images/data_labels_test.csv',\n",
    "                                             transforms=album_compose)\n",
    "\n",
    "# Concatenate training and test sets with both non-attacked images and attacked, with true non-attacked image for both\n",
    "# Used for re-training the model later\n",
    "dataset_train = concat_datasets(dataset_originals_train, dataset_attacks_train)\n",
    "dataset_test  = concat_datasets(dataset_originals_test,  dataset_attacks_test)\n",
    "\n",
    "\n",
    "\n",
    "# Load data into loaders (attack sets have label to what they are predicted to)\n",
    "dataloader_attacks_train   = DataLoader(dataset=dataset_attacks_train,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "dataloader_attacks_test    = DataLoader(dataset=dataset_attacks_test,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "dataloader_originals_train = DataLoader(dataset=dataset_originals_train,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "dataloader_originals_test  = DataLoader(dataset=dataset_originals_test,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "\n",
    "# Create dataloaders for train and test data\n",
    "dataloader_train           = DataLoader(dataset=dataset_train,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "dataloader_test            = DataLoader(dataset=dataset_test,\n",
    "                                        batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063507d",
   "metadata": {},
   "source": [
    "## Load classifier model and classify non-attacked and attacked-images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3c6e6",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4924b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'efficientnet-b0'\n",
    "INIT_LR = 0.01\n",
    "LOAD_PATH = './checkpoints/2021-11-05_400x400_100/models/last.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98028f",
   "metadata": {},
   "source": [
    "#### Load and and classify on non-attacked and attacked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771f9e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ./checkpoints/2021-11-05_400x400_100/models/last.pt\n"
     ]
    }
   ],
   "source": [
    "def test_dataloader(dataloader, model, device, loss_function, optimizer):\n",
    "    test_acc, test_loss, misclassified_images, misclassified_labels, correct_labels = \\\n",
    "        classify.test(dataloader, model, device, loss_function, optimizer)\n",
    "    print('  Accuracy:   '+str(np.around(100*test_acc,3))+'%')\n",
    "\n",
    "    \n",
    "    \n",
    "# Load model and setup\n",
    "model = torch.load(LOAD_PATH)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(lr=INIT_LR, params=model.parameters(), betas=(0.9, 0.99))\n",
    "print('Loaded model:', LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb302d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ./checkpoints/2021-11-05_400x400_100/models/last.pt\n",
      "\n",
      "Attacks train\n",
      "  Accuracy:   100.0%\n",
      "\n",
      "Attacks test\n",
      "  Accuracy:   100.0%\n",
      "\n",
      "Non-attacks train\n",
      "  Accuracy:   95.699%\n",
      "\n",
      "Non-attacks test\n",
      "  Accuracy:   97.5%\n",
      "\n",
      "Concatenated train data\n",
      "  Accuracy:   47.849%\n",
      "\n",
      "Concatenated test data\n",
      "  Accuracy:   48.75%\n"
     ]
    }
   ],
   "source": [
    "import classify\n",
    "\n",
    "\n",
    "\n",
    "# Classify all data sets\n",
    "print('\\nAttacks train')\n",
    "test_dataloader(dataloader_attacks_train, model, device, loss_function, optimizer)\n",
    "print('\\nAttacks test')\n",
    "test_dataloader(dataloader_attacks_test, model, device, loss_function, optimizer)\n",
    "print('\\nNon-attacks train')\n",
    "test_dataloader(dataloader_originals_train, model, device, loss_function, optimizer)\n",
    "print('\\nNon-attacks test')\n",
    "test_dataloader(dataloader_originals_test, model, device, loss_function, optimizer)\n",
    "print('\\nConcatenated train data')\n",
    "test_dataloader(dataloader_train, model, device, loss_function, optimizer)\n",
    "print('\\nConcatenated test data')\n",
    "test_dataloader(dataloader_test, model, device, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef26af8",
   "metadata": {},
   "source": [
    "## Retrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ed170",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c54324",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_RETRAIN = False\n",
    "\n",
    "MODEL_NAME_RETRAIN = 'efficientnet-b0'\n",
    "NUM_CLASSES_RETRAIN = 3\n",
    "EPOCHS_RETRAIN = 10\n",
    "INIT_LR_RETRAIN = 0.01\n",
    "LOAD_PATH_RETRAIN = './checkpoints/2021-12-02_robustness/models/last.pt'\n",
    "SAVE_PATH_RETRAIN = './checkpoints/2021-12-02_robustness'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242243f",
   "metadata": {},
   "source": [
    "#### Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0663e07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training: 2021-12-02 16:57:42.983307\n",
      "\n",
      "Epoch 1     Training accuracy: 90.32%, loss: 0.51468   Saved current model as best model.\n",
      "Epoch 2     Training accuracy: 94.62%, loss: 0.19948   Epoch 3     Training accuracy: 97.85%, loss: 0.06842   Epoch 4     Training accuracy: 98.39%, loss: 0.04012   Epoch 5     Training accuracy: 99.73%, loss: 0.0145   Epoch 6     Training accuracy: 100.0%, loss: 0.00572   Epoch 7     Training accuracy: 100.0%, loss: 0.00319   Epoch 8     Training accuracy: 100.0%, loss: 0.00575   Epoch 9     Training accuracy: 100.0%, loss: 0.00393   Epoch 10    Training accuracy: 100.0%, loss: 0.00413   Saved last model.\n",
      "\n",
      "Ended: 2021-12-02 17:44:07.876009\n",
      "Total training time: 0:46:24.892267\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from utils import create_save_path\n",
    "import classify\n",
    "from models.efficientnet import EfficientNet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "# Initialize value for keeping track of \"best\" model with highest validation accuracy\n",
    "best_monitor_value = 0\n",
    "\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "if LOAD_MODEL_RETRAIN:\n",
    "    \n",
    "    # Setup\n",
    "    retrained_model = torch.load(LOAD_PATH_RETRAIN)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    retrained_model.to(device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(lr=INIT_LR_RETRAIN, params=retrained_model.parameters(), betas=(0.9, 0.99))\n",
    "    \n",
    "    # Print information\n",
    "    print('Loaded model:', LOAD_PATH_RETRAIN)\n",
    "\n",
    "    \n",
    "\n",
    "# Train model\n",
    "else:\n",
    "    \n",
    "    # Setup device, model, loss function, and optimizer\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    retrained_model = copy.deepcopy(model)\n",
    "    retrained_model.to(device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    loss_function.to(device)\n",
    "    optimizer = torch.optim.Adam(lr=INIT_LR_RETRAIN, params=retrained_model.parameters(), betas=(0.9, 0.99))\n",
    "    optimizer_step = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_RETRAIN, eta_min=1e-5)\n",
    "\n",
    "    # Create path for saving training log and models\n",
    "    train_log_path, models_path = create_save_path(SAVE_PATH_RETRAIN)\n",
    "\n",
    "    # Initialize log writer\n",
    "    log_writer = SummaryWriter(train_log_path, comment=f'LR_{INIT_LR_RETRAIN}_BS_{BATCH_SIZE}')\n",
    "\n",
    "    # Print starting time\n",
    "    time_start = time.time()\n",
    "    print('Started training:', str(datetime.now()))\n",
    "    print('')\n",
    "    \n",
    "    valid_acc = 0\n",
    "    \n",
    "    # Train for each epoch\n",
    "    for epoch in range(EPOCHS_RETRAIN):\n",
    "        \n",
    "        # Print current epoch\n",
    "        print('Epoch {:<3}'.format(epoch+1), end='   ')\n",
    "        \n",
    "        # Train model\n",
    "        train_acc, train_loss = classify.train(dataloader_train, retrained_model, device, loss_function, optimizer)\n",
    "        print('Training accuracy: {:>3}%, loss: {:<5}'.format(\n",
    "            np.around(100*train_acc, 2), np.around(train_loss, 5)), end='\\n')\n",
    "\n",
    "        # Optimize\n",
    "        optimizer_step.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # We do not have validation set here\n",
    "        valid_acc = valid_acc + 1\n",
    "\n",
    "        # Save model if better than previously best model as well as the last model\n",
    "        best_monitor_value = classify.save_model(model=retrained_model, epoch=epoch, best_monitor_value=best_monitor_value,\n",
    "                                                 monitor_value=valid_acc, epochs=EPOCHS_RETRAIN, models_path=models_path)\n",
    "\n",
    "        # Write to tensorboard log\n",
    "        log_writer.add_scalar(\"Train/Training accuracy\",   train_acc,  epoch)\n",
    "        log_writer.add_scalar(\"Train/Training loss\",       train_loss, epoch)\n",
    "        log_writer.add_scalar(\"Train/Learning rate\",       lr,         epoch)\n",
    "\n",
    "    # Print time elapsed\n",
    "    time_elapsed = time.time() - time_start\n",
    "    print('')\n",
    "    print('Ended:', str(datetime.now()))\n",
    "    print('Total training time: {}'.format(str(timedelta(seconds=time_elapsed))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f2323",
   "metadata": {},
   "source": [
    "#### Try to classify all data sets again after retraining of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e53bae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attacks train\n",
      "  Accuracy:   4.301%\n",
      "\n",
      "Attacks test\n",
      "  Accuracy:   6.25%\n",
      "\n",
      "Non-attacks train\n",
      "  Accuracy:   97.312%\n",
      "\n",
      "Non-attacks test\n",
      "  Accuracy:   95.0%\n",
      "\n",
      "Concatenated train data\n",
      "  Accuracy:   96.505%\n",
      "\n",
      "Concatenated test data\n",
      "  Accuracy:   94.375%\n"
     ]
    }
   ],
   "source": [
    "# Classify all data sets again after retraining\n",
    "print('\\nAttacks train')\n",
    "test_dataloader(dataloader_attacks_train, retrained_model, device, loss_function, optimizer)\n",
    "print('\\nAttacks test')\n",
    "test_dataloader(dataloader_attacks_test, retrained_model, device, loss_function, optimizer)\n",
    "print('\\nNon-attacks train')\n",
    "test_dataloader(dataloader_originals_train, retrained_model, device, loss_function, optimizer)\n",
    "print('\\nNon-attacks test')\n",
    "test_dataloader(dataloader_originals_test, retrained_model, device, loss_function, optimizer)\n",
    "print('\\nConcatenated train data')\n",
    "test_dataloader(dataloader_train, retrained_model, device, loss_function, optimizer)\n",
    "print('\\nConcatenated test data')\n",
    "test_dataloader(dataloader_test, retrained_model, device, loss_function, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
